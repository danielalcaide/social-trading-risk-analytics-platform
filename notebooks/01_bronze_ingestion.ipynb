{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1ebaef0-4ec9-4264-ab31-f8fdc36d4e48",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Bronze Ingestion Pipeline: Social Trading Risk Analysis Platform\n",
    "\n",
    "**Purpose:** Bronze layer ingestion - Extract raw trader profile data from social trading platforms\n",
    "\n",
    "**Architecture Decision:** \n",
    "- Store raw HTML responses in Delta Lake for full reprocessability\n",
    "- Immutable bronze layer - never update, only append\n",
    "- Preserves source data even if platform changes structure\n",
    "\n",
    "**Key Technical Challenges:**\n",
    "1. **Rate Limiting:** Respectful scraping with proxy rotation + random delays\n",
    "2. **Idempotency:** Anti-join pattern prevents duplicate processing\n",
    "3. **Error Handling:** Store HTTP status for debugging failed requests\n",
    "\n",
    "**Output:** `bronze_trader_profiles`- Delta table containing raw data (Bronze Layer)\n",
    "- Schema: `user`, `timestamp`, `key`, `url`, `status`, `response`\n",
    "- Partitioned by date for efficient time-travel queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "128ab48a-9461-48fc-977a-7185fb5a5057",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1. Environment **Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ade23458-3f44-45f4-8b76-11aac9e5f591",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install python-dotenv\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1b3af48-de62-4401-b307-b1a0016712b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2. Proxy Configuration\n",
    "\n",
    "**Technical Decision: Why Proxy Rotation?**\n",
    "\n",
    "When extracting data from web sources at scale, rate limiting is the primary technical challenge:\n",
    "- Single IP: Limited to ~50-100 requests before throttling\n",
    "- Solution: Rotate through a pool of 10 proxy IPs\n",
    "\n",
    "This enables respectful data collection while maintaining reliability:\n",
    "- Random delays (5-20s) between requests\n",
    "- Distributed load across proxy pool\n",
    "- HTTP status tracking for monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1351e3a0-2bd7-4cc9-9828-43a819997c78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "env_path = os.path.join(\n",
    "    os.path.dirname(os.getcwd()),\n",
    "    '.env'\n",
    ")\n",
    "load_dotenv(env_path)\n",
    "\n",
    "# Configuration Proxy\n",
    "API_TOKEN = os.getenv(\"PROXY_API_TOKEN\")\n",
    "PARAMS = json.loads(os.getenv(\"PROXY_API_PARAMS\"))\n",
    "URL = os.getenv(\"PROXY_API_URL\")\n",
    "\n",
    "# Request list of proxies\n",
    "response = requests.get(URL, headers={ \"Authorization\": API_TOKEN }, params=PARAMS)\n",
    "\n",
    "if response.status_code != 200:\n",
    "    raise Exception(\"Something went wrong in the answer of the server\")\n",
    "\n",
    "print(f\"Identified {response.json()['count']} proxies\")\n",
    "\n",
    "proxy_list = response.json()['results']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5789691-9c9e-4060-913b-99031ce372b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3. Scraping Functions\n",
    "\n",
    "**Design Pattern: Separation of Concerns**\n",
    "\n",
    "This module handles ONLY data acquisition:\n",
    "- `fetch_trader_profile_data()`: HTTP request with proxy rotation\n",
    "- `get_pending_traders()`: Determines which users need processing (idempotency)\n",
    "- `get_downloaded_traders()`: Metrics for monitoring progress\n",
    "- `get_url_list()`: List of URL for each trader\n",
    "\n",
    "**Parsing logic is deliberately separated** - Bronze layer stores raw HTML, \n",
    "Silver layer handles parsing. This allows:\n",
    "- Reprocessing if parsing logic changes\n",
    "- A/B testing different extraction strategies\n",
    "- Full audit trail of source data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06e483c6-b396-4072-adf9-4b02529e9fe2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "import random\n",
    "\n",
    "BRONZE_TRADER_PROFILES = os.getenv(\"BRONZE_TRADER_PROFILES\")\n",
    "PLATFORM_BASE_URL = os.getenv(\"PLATFORM_BASE_URL\")\n",
    "\n",
    "def fetch_trader_profile_data(url, use_proxy=True, proxy_list = None):\n",
    "    \"\"\"\n",
    "    Fetch trader profile data from social trading platform\n",
    "    \n",
    "    Args:\n",
    "        url (str): Profile page URL\n",
    "        use_proxy (bool): Enable proxy rotation for rate limit management\n",
    "        proxy_list (list): Pool of proxy configurations\n",
    "        \n",
    "    Returns:\n",
    "        requests.Response: HTTP response containing profile HTML\n",
    "        \n",
    "    Note: This implementation uses web scraping as the platform does not\n",
    "    provide a public API for bulk data extraction. In production systems,\n",
    "    official API access is always preferred when available.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add headers to mimic a real browser\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "    \n",
    "    # Proxy configuration\n",
    "    if use_proxy:\n",
    "\n",
    "        # Randomly select a proxy\n",
    "        selected_proxy = random.choice(proxy_list)\n",
    "        \n",
    "        # Format proxy URL\n",
    "        proxy_url = f\"http://{selected_proxy['username']}:{selected_proxy['password']}@{selected_proxy['proxy_address']}:{selected_proxy['port']}\"\n",
    "        \n",
    "        proxies = {\n",
    "            'http': proxy_url,\n",
    "            'https': proxy_url,\n",
    "        }\n",
    "        print(f\"Using proxy: {proxy_url}\")\n",
    "        response = requests.get(url, headers=headers, proxies=proxies)\n",
    "    else:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        \n",
    "    return response\n",
    "\n",
    "def get_pending_traders():\n",
    "    \"\"\"\n",
    "    Identify traders pending data collection (idempotency logic)\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Trader usernames not yet processed in last 5 days\n",
    "        \n",
    "    Implementation:\n",
    "        - LEFT ANTI JOIN: source list NOT IN recent ingestions\n",
    "        - 5-day refresh window for profile updates\n",
    "        - Prevents duplicate processing within time window\n",
    "    \"\"\"\n",
    "    users = pd.read_csv('../data/traders/users_tier1.txt', sep='\\r', header=None)\n",
    "    users.columns = ['user']\n",
    "    \n",
    "    existing_users = spark.table(BRONZE_TRADER_PROFILES) \\\n",
    "    .select(['user', 'timestamp']) \\\n",
    "    .where('timestamp > date_sub(current_date(), 5)') \\\n",
    "    .select(['user']) \\\n",
    "    .distinct() \\\n",
    "    .toPandas()\n",
    "    \n",
    "    return users[~users['user'].isin(existing_users['user'])]\n",
    "\n",
    "def get_downloaded_traders (days = 0):\n",
    "    \"\"\"\n",
    "    Count distinct users processed in last N days (monitoring metric)\n",
    "    \n",
    "    Args:\n",
    "        days (int): Lookback window (0 = today only)\n",
    "        \n",
    "    Returns:\n",
    "        int: Count of distinct users downloaded\n",
    "    \"\"\"\n",
    "\n",
    "    return spark.table(BRONZE_TRADER_PROFILES) \\\n",
    "    .select(['user', 'timestamp']) \\\n",
    "    .where(f'timestamp > date_sub(current_date(), {days})') \\\n",
    "    .select(['user']) \\\n",
    "    .distinct() \\\n",
    "    .count()\n",
    "\n",
    "def get_url_list(username):\n",
    "    \"\"\"\n",
    "    Generate URLs for data extraction endpoints\n",
    "    \n",
    "    Note: Currently only fetching 'factsheet' endpoint\n",
    "    Future: Can enable overview, trades, history for richer dataset\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"factsheet\": f\"{PLATFORM_BASE_URL}/factsheet/{username}\"\n",
    "    }\n",
    "    \n",
    "    # Commented: Additional endpoints available but not used yet\n",
    "    # return {\n",
    "    #     \"overview\": f\"{PLATFORM_BASE_URL}/etoro/{username}\",\n",
    "    #     \"trades\": f\"{PLATFORM_BASE_URL}/etoro/{username}/trades\",\n",
    "    #     \"history\": f\"{PLATFORM_BASE_URL}/etoro/{username}/history\",\n",
    "    #     \"factsheet\": f\"{PLATFORM_BASE_URL}/factsheet/{username}\"\n",
    "    # }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3169705-3b1c-4f3f-a13c-99ac9ca2af34",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4. Main Ingestion Loop\n",
    "\n",
    "**Control Flow Logic:**\n",
    "\n",
    "```\n",
    "WHILE (pending_users > threshold):\n",
    "  1. Sample 20 random users (prevents sequential patterns)\n",
    "  2. For each user:\n",
    "     - Scrape data with random proxy\n",
    "     - Store raw HTML in Delta Lake (append-only)\n",
    "     - Sleep 5-20 seconds (respectful rate limiting)\n",
    "  3. Sleep 1-3 minutes between batches (avoid burst patterns)\n",
    "```\n",
    "\n",
    "**Why This Design?**\n",
    "- **Random sampling:** Distributes load, avoids detection patterns\n",
    "- **Random delays:** Mimics human browsing behavior\n",
    "- **Batch processing:** Efficient while maintaining rate limits\n",
    "- **Delta append:** Immutable bronze layer, no updates\n",
    "\n",
    "**Monitoring:**\n",
    "- Target: Process 20% of user list per run\n",
    "- Progress: `users_downloaded(days=0)` tracks daily progress\n",
    "- Logs: Print statements for debugging (can be enhanced with MLflow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41a8820c-c8cb-4485-a96f-79d0d0be123a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import random\n",
    "import time\n",
    "\n",
    "while len(get_pending_traders()) * 0.2 - get_downloaded_traders(days=0) > 1:\n",
    "\n",
    "    users = get_pending_traders().sample(20)\n",
    "\n",
    "    for user in users['user']:\n",
    "        print(user)\n",
    "        urls = get_url_list(user)\n",
    "        print(urls)\n",
    "        for key, url in urls.items():\n",
    "            data = []\n",
    "            response = fetch_trader_profile_data(url, proxy_list=proxy_list)\n",
    "            timestamp = datetime.datetime.now()\n",
    "            \n",
    "            data.append({\n",
    "                'user': user,\n",
    "                'timestamp': timestamp,\n",
    "                'key': key,\n",
    "                'url': url,\n",
    "                'status': response.status_code,\n",
    "                'response': response.text\n",
    "            })\n",
    "            \n",
    "            # Printing\n",
    "            print(key)\n",
    "            print(response.status_code)\n",
    "            \n",
    "            # Convert the list of dictionaries to a Spark DataFrame\n",
    "            df = spark.createDataFrame(data)\n",
    "\n",
    "            # Write the DataFrame to a Delta table in the default schema\n",
    "            df.write.format(\"delta\").mode(\"append\").saveAsTable(BRONZE_TRADER_PROFILES)\n",
    "            sleep_duration = random.randint(5, 20)\n",
    "            print(f\"Sleeping for {sleep_duration} seconds\")\n",
    "            time.sleep(sleep_duration)\n",
    "            print(\"-\"*10)\n",
    "\n",
    "    sleep_duration = random.randint(60, 3*60)\n",
    "    print(f\"Sleeping for {sleep_duration} seconds between requests\")\n",
    "    print(\"*\"*10)\n",
    "    time.sleep(sleep_duration)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01_bronze_ingestion",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
